//! Diff-history scan pipeline.
//!
//! Walks commits in topological order, diffs trees to discover changed blobs,
//! spills and deduplicates candidates, then batch-plans and executes pack
//! decode + scan.
//!
//! # Pipeline stages
//!
//! ```text
//! PlannedCommits ──► TreeDiff ──► Spill/Dedupe ──► MappingBridge ──► PackPlan ──► PackExec
//!   (topo order)    (blob OIDs)   (seen filter)    (OID→pack loc)   (per-pack)   (decode+scan)
//! ```
//!
//! 1. **Tree diff** – walks each planned commit, diffs its tree against parent
//!    trees (or the empty tree for snapshot roots) to discover changed blob
//!    OIDs. Candidates stream directly into the spiller to bound memory.
//! 2. **Spill / dedupe** – the spiller externalizes candidates to disk when the
//!    in-memory budget is exceeded, then replays them through the seen-blob
//!    store to drop already-scanned OIDs.
//! 3. **Mapping bridge** – maps surviving OIDs to pack locations via the MIDX,
//!    partitioning into packed and loose candidate sets. Produces a `ByteArena`
//!    of file paths that pack-exec and loose-scan reference by `ByteRef`.
//! 4. **Pack planning** – groups packed candidates by pack file and builds a
//!    topologically-sorted decode plan that respects delta chains.
//! 5. **Pack execution** – decodes pack entries, resolves deltas, and feeds
//!    reconstructed blobs to the detection engine for scanning.
//!
//! # Batch planning
//!
//! Unlike the ODB-blob path, all pack plans are built before execution begins.
//! This enables a pre-execution artifact staleness check: if `git gc` or
//! `git repack` ran between planning and execution, the function bails with
//! `ConcurrentMaintenance` rather than reading corrupt pack offsets.
//!
//! # Execution strategies
//!
//! The `PackExecStrategy` enum selects one of three modes based on worker
//! count and plan structure:
//!
//! | Strategy | Condition | Parallelism |
//! |---|---|---|
//! | `Serial` | 1 worker, 0 plans, or tiny total decode work | Scheduler executes pack tasks with 1 worker |
//! | `PackParallel` | enough plans to saturate workers | Scheduler executes pack tasks with `pack_exec_workers` |
//! | `IntraPackSharded` | fewer plans than workers, non-trivial decode work | Scheduler executes pack tasks with `pack_exec_workers` |
//!
//! All three strategies produce **deterministic output order** by reassembling
//! results in planned sequence, regardless of worker completion order.

use std::io;
use std::sync::Arc;
use std::time::Instant;

use crate::scheduler::{alloc_stats, AllocStats};
use crate::Engine;

use super::commit_walk::{CommitGraph, ParentScratch, PlannedCommit};
use super::engine_adapter::{EngineAdapter, ScannedBlobs};
use super::mapping_bridge::{MappingBridge, MappingBridgeConfig};
use super::object_store::ObjectStore;
use super::pack_candidates::CappedPackCandidateSink;
use super::pack_io::PackIo;
use super::pack_plan::build_pack_plans;
use super::policy_hash::MergeDiffMode;
use super::repo_open::RepoJobState;
use super::runner::{
    GitScanAllocStats, GitScanConfig, GitScanError, GitScanStageNanos, ScanModeOutput,
};
use super::seen_store::SeenBlobStore;
use super::spiller::Spiller;
use super::tree_delta_cache::TreeDeltaCache;
use super::tree_diff::TreeDiffWalker;

use super::runner_exec::{
    append_scanned_blobs, auto_tree_delta_cache_bytes, build_pack_views, collect_loose_dirs,
    collect_pack_dirs, execute_pack_plans_with_scheduler, list_pack_files, load_midx,
    make_spill_dir, mmap_pack_files, per_worker_cache_bytes, resolve_pack_paths,
    scan_loose_candidates, select_pack_exec_strategy, summarize_pack_plan_deps, PackExecStrategy,
    SpillCandidateSink,
};

/// Runs the diff-history scan pipeline.
///
/// Walks planned commits, diffs trees to discover changed blobs, streams
/// candidates through the spill/dedupe pipeline, then batch-plans and
/// executes pack decode + scan. See the [module docs](self) for the full
/// pipeline diagram and execution strategy selection.
///
/// The caller (dispatcher) is responsible for finalize, persist, and perf
/// snapshot on the returned output.
///
/// # Determinism
///
/// Output order is deterministic for identical inputs regardless of worker
/// count. Parallel strategies reassemble results by planned sequence index,
/// and loose candidates are always appended after all packed results.
///
/// # Lifetime of the returned path arena
///
/// The `path_arena` in the returned [`ScanModeOutput`] owns all file-path
/// bytes referenced by `ByteRef` handles in the scanned blobs and skipped
/// candidates. It must outlive any downstream consumer that dereferences
/// those handles.
///
/// # Errors
///
/// - MIDX load, completeness, or OID resolution failures.
/// - Pack plan errors (out-of-range pack IDs, corrupt headers).
/// - Pack exec errors (decode failures, delta resolution, I/O).
/// - `GitScanError::ConcurrentMaintenance` if repository artifacts (pack
///   files, indices) changed between pack planning and pack execution,
///   indicating a concurrent `git gc` or `git repack`.
pub(super) fn run_diff_history(
    repo: &RepoJobState,
    engine: Arc<Engine>,
    seen_store: &dyn SeenBlobStore,
    cg: &dyn CommitGraph,
    plan: &[PlannedCommit],
    config: &GitScanConfig,
    event_sink: std::sync::Arc<dyn crate::unified::events::EventSink>,
) -> Result<ScanModeOutput, GitScanError> {
    let mut stage_nanos = GitScanStageNanos::default();
    let mut alloc_deltas = GitScanAllocStats::default();

    // Spill + dedupe (stream candidates during tree diff).
    // Shared spill directory for tree payloads and pack-exec large blobs.
    let spill_dir = match &config.spill_dir {
        Some(path) => path.clone(),
        None => make_spill_dir()?,
    };

    let mut spiller = Spiller::new(config.spill, repo.object_format.oid_len(), &spill_dir)?;
    let midx = load_midx(repo)?;
    // Scale tree delta-cache to repo size to reduce repeated base decode work
    // while staying within the configured max cache budget.
    let auto_cache_bytes = auto_tree_delta_cache_bytes(
        midx.object_count(),
        config.tree_diff.max_tree_delta_cache_bytes,
    );
    let tree_delta_cache = TreeDeltaCache::new(auto_cache_bytes);
    let mut object_store = ObjectStore::open_with_tree_delta_cache(
        repo,
        &config.tree_diff,
        &spill_dir,
        tree_delta_cache,
    )?;
    let mut walker = TreeDiffWalker::new(&config.tree_diff, repo.object_format.oid_len());
    // Reused across commits to avoid per-commit allocation for parent position lists.
    let mut parent_scratch = ParentScratch::new();

    // ── Stage 1: Tree diff ──────────────────────────────────────────────
    // Walk each planned commit and diff its tree against parent trees to
    // discover changed blob OIDs. The SpillCandidateSink streams candidates
    // directly into the spiller, bounding memory even for repositories with
    // millions of changed blobs.
    {
        let diff_start = Instant::now();
        let mut sink = SpillCandidateSink::new(&mut spiller);
        for PlannedCommit { pos, snapshot_root } in plan {
            let commit_id = pos.0;
            let new_tree = cg.root_tree_oid(*pos)?;

            // Snapshot roots are diffed against the empty tree (old_tree=None),
            // treating every blob in the tree as a new addition. This is used
            // for orphan commits and forced snapshot points in the commit plan.
            if *snapshot_root {
                walker.diff_trees(
                    &mut object_store,
                    &mut sink,
                    Some(&new_tree),
                    None,
                    commit_id,
                    0,
                )?;
                continue;
            }

            parent_scratch.clear();
            cg.collect_parents(
                *pos,
                config.commit_walk.max_parents_per_commit,
                &mut parent_scratch,
            )?;
            let parents = parent_scratch.as_slice();

            // Parentless commits (e.g. grafted roots) use the same empty-tree
            // diff as snapshot roots—every blob is considered introduced.
            if parents.is_empty() {
                walker.diff_trees(
                    &mut object_store,
                    &mut sink,
                    Some(&new_tree),
                    None,
                    commit_id,
                    0,
                )?;
                continue;
            }

            match config.merge_diff_mode {
                MergeDiffMode::AllParents => {
                    for (idx, parent_pos) in parents.iter().enumerate() {
                        let old_tree = cg.root_tree_oid(*parent_pos)?;
                        walker.diff_trees(
                            &mut object_store,
                            &mut sink,
                            Some(&new_tree),
                            Some(&old_tree),
                            commit_id,
                            idx as u8,
                        )?;
                    }
                }
                MergeDiffMode::FirstParentOnly => {
                    let old_tree = cg.root_tree_oid(parents[0])?;
                    walker.diff_trees(
                        &mut object_store,
                        &mut sink,
                        Some(&new_tree),
                        Some(&old_tree),
                        commit_id,
                        0,
                    )?;
                }
            }
        }
        stage_nanos.tree_diff = diff_start.elapsed().as_nanos() as u64;
    }

    // ── Stages 2–3: Spill/dedupe → mapping bridge ──────────────────────
    // Finalize the spiller: flush any on-disk runs, merge-sort, and replay
    // unique (unseen) OIDs through the mapping bridge. The bridge resolves
    // each OID to a pack location via the MIDX, splitting candidates into
    // packed (with pack_id + offset) and loose (fan-out directory lookup).
    // The returned path arena owns all file-path bytes for the rest of the
    // pipeline.
    let spill_start = Instant::now();
    let mut mapping_cfg = config.mapping;
    let default_mapping_cfg = MappingBridgeConfig::default();
    if mapping_cfg.max_packed_candidates >= default_mapping_cfg.max_packed_candidates {
        // Keep explicit low caps as hard limits; only scale default-or-higher
        // caps to handle very large repositories.
        mapping_cfg.max_packed_candidates =
            mapping_cfg.max_packed_candidates.max(midx.object_count());
    }
    let mut bridge = MappingBridge::new(
        &midx,
        CappedPackCandidateSink::new(
            mapping_cfg.max_packed_candidates,
            mapping_cfg.max_loose_candidates,
        ),
        mapping_cfg,
    );
    let spill_stats = spiller.finalize(seen_store, &mut bridge)?;
    stage_nanos.spill = spill_start.elapsed().as_nanos() as u64;
    let (mapping_stats, mut sink, mapping_arena) = bridge.finish()?;
    let mapping_arena = Arc::new(mapping_arena);

    // ── Stage 4: Pack planning ───────────────────────────────────────────
    let pack_plan_start = Instant::now();
    let pack_dirs = collect_pack_dirs(&repo.paths);
    let pack_names = list_pack_files(&pack_dirs)?;
    midx.verify_completeness(pack_names.iter().map(|n| n.as_slice()))?;
    let pack_paths = resolve_pack_paths(&midx, &pack_dirs)?;
    // Only mmap packs that contain at least one candidate. This avoids
    // wasting address space on multi-pack repos where most packs are idle.
    let mut used_pack_ids: Vec<u16> = sink.packed.iter().map(|cand| cand.pack_id).collect();
    used_pack_ids.sort_unstable();
    used_pack_ids.dedup();
    let pack_mmaps = mmap_pack_files(&pack_paths, &used_pack_ids, config.pack_mmap)?;
    let pack_views = build_pack_views(&pack_mmaps, repo.object_format)?;

    let mut pack_plan_stats = Vec::new();
    let mut plans = Vec::new();
    let packed_len = sink.packed.len();
    let loose_len = sink.loose.len();
    if !sink.packed.is_empty() {
        // Move candidates out of the sink to avoid cloning the potentially
        // large candidate vec; the sink's packed field is left empty.
        let packed = std::mem::take(&mut sink.packed);
        let mut pack_plans = build_pack_plans(packed, &pack_views, &midx, &config.pack_plan)?;
        pack_plan_stats.extend(pack_plans.iter().map(|p| p.stats));
        plans.append(&mut pack_plans);
    }
    let (pack_plan_delta_deps_total, pack_plan_delta_deps_max) = summarize_pack_plan_deps(&plans);
    stage_nanos.pack_plan = pack_plan_start.elapsed().as_nanos() as u64;

    // Gate between planning and execution: if pack files or indices were
    // rewritten by a concurrent `git gc` / `git repack`, the offsets in our
    // plans are stale. Bail early rather than reading garbage.
    if !repo.artifacts_unchanged()? {
        return Err(GitScanError::ConcurrentMaintenance);
    }

    // ── Stage 5: Pack execution + scan ───────────────────────────────────
    let pack_cache_target = per_worker_cache_bytes(
        config.pack_cache_bytes,
        &pack_mmaps,
        &used_pack_ids,
        config.pack_exec_workers,
    );
    let pack_cache_bytes: u32 = pack_cache_target
        .try_into()
        .map_err(|_| io::Error::other("pack cache size exceeds u32::MAX"))?;
    let pack_exec_workers = config.pack_exec_workers.max(1);
    let pack_exec_strategy = select_pack_exec_strategy(pack_exec_workers, &plans);
    let plan_count = plans.len();
    let loose_dirs = Arc::new(collect_loose_dirs(&repo.paths));
    let pack_paths = Arc::new(pack_paths);
    let pack_mmaps = Arc::new(pack_mmaps);
    let spill_dir = Arc::new(spill_dir);
    let mut pack_exec_reports = Vec::with_capacity(plan_count);
    let mut skipped_candidates = Vec::new();
    let mut scanned = ScannedBlobs {
        blobs: Vec::with_capacity(packed_len.saturating_add(loose_len)),
        finding_arena: Vec::new(),
    };

    let pack_exec_start = Instant::now();
    let pack_exec_alloc_before: AllocStats = alloc_stats();
    if !plans.is_empty() {
        let scheduler_workers = match pack_exec_strategy {
            PackExecStrategy::Serial => 1,
            PackExecStrategy::PackParallel | PackExecStrategy::IntraPackSharded { .. } => {
                pack_exec_workers
            }
        };
        let midx_bytes = repo
            .mmaps
            .midx
            .clone()
            .ok_or_else(|| GitScanError::Io(io::Error::other("midx bytes missing")))?;
        let outputs = execute_pack_plans_with_scheduler(
            Arc::clone(&engine),
            event_sink.clone(),
            midx_bytes,
            repo.object_format,
            Arc::clone(&pack_paths),
            Arc::clone(&loose_dirs),
            Arc::clone(&pack_mmaps),
            Arc::clone(&mapping_arena),
            Arc::clone(&spill_dir),
            plans,
            config.pack_decode,
            config.pack_io,
            config.engine_adapter,
            pack_cache_bytes,
            scheduler_workers,
        )?;
        for output in outputs {
            pack_exec_reports.push(output.report);
            skipped_candidates.extend(output.skipped);
            append_scanned_blobs(&mut scanned, output.scanned);
        }
    }
    if !sink.loose.is_empty() {
        let mut adapter = EngineAdapter::new_with_event_sink(
            engine.as_ref(),
            config.engine_adapter,
            event_sink.clone(),
        );
        adapter.reserve_results(sink.loose.len());
        let mut external = PackIo::from_parts(
            midx,
            (*pack_paths).clone(),
            (*loose_dirs).clone(),
            config.pack_io,
        )
        .map_err(GitScanError::PackIo)?;
        scan_loose_candidates(
            &sink.loose,
            mapping_arena.as_ref(),
            &mut adapter,
            &mut external,
            &mut skipped_candidates,
        )?;
        append_scanned_blobs(&mut scanned, adapter.take_results());
    }
    stage_nanos.pack_exec = pack_exec_start.elapsed().as_nanos() as u64;
    let pack_exec_alloc_after = alloc_stats();
    alloc_deltas.pack_exec = pack_exec_alloc_after.since(&pack_exec_alloc_before);

    Ok(ScanModeOutput {
        scanned,
        path_arena: Arc::try_unwrap(mapping_arena)
            .map_err(|_| GitScanError::Io(io::Error::other("path arena still shared")))?,
        skipped_candidates,
        pack_exec_reports,
        pack_plan_stats,
        pack_plan_config: config.pack_plan,
        pack_plan_delta_deps_total,
        pack_plan_delta_deps_max,
        tree_diff_stats: walker.stats().clone(),
        spill_stats,
        mapping_stats,
        stage_nanos,
        alloc_stats: alloc_deltas,
        pack_cache_per_worker_bytes: pack_cache_target,
    })
}
