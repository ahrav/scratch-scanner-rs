# Git Pack Exec Optimization Plan (Consolidated)

This document consolidates and stack-ranks all identified pack execution optimizations. Each item is self-contained with sufficient context to implement independently.

---

## Baseline Performance Results

**Date:** 2025-02-05
**Build:** `RUSTFLAGS="-C target-cpu=native" cargo build --release --features git-perf`

### Workload Comparison

| Repository | Blobs  | decode             | cache_lookup    | fallback_resolve | sink_emit           | cache_hit_rate | fallback_rate |
|------------|--------|--------------------|-----------------|------------------|---------------------|----------------|---------------|
| gitleaks   | 5,924  | 181.1% (0.26s)     | 0.2% (0.00s)    | 93.0% (0.14s)    | 749.8% (1.09s)      | 0.0%           | 14.4%         |
| go-git     | 9,550  | 310.1% (0.32s)     | 1.6% (0.00s)    | 211.8% (0.22s)   | 654.6% (0.68s)      | 0.0%           | 17.7%         |
| rocksdb    | 65,271 | 503.8% (11.69s)    | 0.5% (0.01s)    | 313.9% (7.28s)   | 1040.8% (24.14s)    | 0.0%           | 21.0%         |

**Note:** Percentages can exceed 100% because pack_exec runs in parallelâ€”timing values are aggregated across all worker threads while stage time is wall-clock.

### Post-Fix Results (After Item 2: Cache Admission Policy Fix + Instrumentation Fix)

**Date:** 2025-02-05
**Changes:**
1. Removed restrictive admission policy that only cached delta bases
2. Fixed instrumentation to measure the **correct** cache hit rate (delta base lookups, not top-level blob lookups)

| Repository | Blobs  | decode             | cache_lookup    | fallback_resolve   | sink_emit           | base_cache_hit_rate | fallback_rate |
|------------|--------|--------------------|-----------------|--------------------|--------------------|---------------------|---------------|
| gitleaks   | 5,924  | 188.6% (0.28s)     | 0.2% (0.00s)    | 87.5% (0.13s)      | 587.0% (0.88s)     | **70.9%** (2933/4137)   | 14.5%         |
| go-git     | 9,550  | 359.5% (0.36s)     | 0.4% (0.00s)    | 259.0% (0.26s)     | 759.9% (0.77s)     | **67.0%** (5489/8189)   | 18.0%         |
| rocksdb    | 65,271 | 517.1% (12.00s)    | 0.1% (0.00s)    | 350.0% (8.12s)     | 990.7% (22.99s)    | **56.7%** (33758/59512) | 22.8%         |

**Key findings:**
- âœ… **Cache IS working effectively** â€” 57-71% hit rate on delta base lookups
- âœ… The original "0% hit rate" was a **measurement bug** â€” it measured top-level blob lookups (always miss by design) instead of delta base lookups
- âœ… Cache saves significant decode work â€” without it, `fallback_resolve` times would roughly double
- ðŸ“Š `sink_emit` remains the dominant bottleneck (590-1000% of wall-clock time)

**Root cause of the measurement bug:** The original `cache_hits` counter only tracked whether a blob was already fully decoded when we started processing it (always 0 since each blob is processed once). The **useful** cache lookups happen when resolving delta bases, which weren't being counted.

### Key Observations

1. **sink_emit dominates** â€” Consistently 590-1000% of wall-clock time, making it the largest time consumer across all workloads. This is the engine scanning phase and was not originally identified as a bottleneck.

2. **Cache is working well** â€” 49-71% base cache hit rate across all 7 repos (57-71% for small/medium repos, 49-51% for large repos). The cache effectively reduces redundant delta base decoding.

3. **fallback_rate is 7-23%** â€” A significant portion of decoded offsets require fallback base resolution, contributing 7+ seconds on rocksdb and 33-90s on larger repos (node, vscode, tensorflow).

4. **decode is significant but not dominant** â€” 180-500% of wall-clock time, representing the actual inflate/delta-apply work.

5. **cache_lookup is negligible** â€” < 2% in all cases, so optimizing cache lookup speed would have minimal impact.

### Revised Priority Analysis

Based on these results, we should reconsider our stack ranking:

| Original Rank | Item                               | Impact Assessment                                       | Recommended Action              |
|---------------|------------------------------------|---------------------------------------------------------|---------------------------------|
| 1             | Instrumentation                    | **DONE**                                                | âœ… Completed                    |
| 2             | Cache admission policy             | **DONE** â€” Fixed admission + instrumentation            | âœ… Completed (cache working at 57-71% hit rate) |
| 3             | Pack-level vs intra-pack heuristic | Medium â€” parallelism appears to be working              | Lower priority                  |
| 4             | Cluster-based sharding             | Lower â€” cache is already effective                      | De-scoped (model removed)       |
| 5             | Batch external base resolution     | Medium â€” fallback_resolve is 8+ seconds on large repos  | Keep rank                       |
| **NEW**       | **sink_emit optimization**         | **CRITICAL** â€” dominates all workloads (590-1000%)      | **Top priority**                |

### Immediate Next Steps

1. ~~**Investigate 0% cache hit rate**~~ â€” âœ… **RESOLVED**: Was a measurement bug. Cache is working at 57-71% hit rate.

2. **Profile sink_emit** â€” This is now the clear bottleneck. Need breakdown of what's happening in the emit path:
   - Engine scan time vs other overhead
   - Memory allocation patterns
   - Any blocking operations

---

**Source files:**
- `src/git_scan/runner.rs` - Pack execution scheduling, sharding, worker coordination
- `src/git_scan/pack_exec.rs` - Decode loop, cache interactions, delta resolution
- `src/git_scan/pack_plan.rs` - Execution order and delta dependency graph
- `src/git_scan/pack_plan_model.rs` - Data structures (PackPlan, DeltaDep)
- `src/git_scan/pack_cache.rs` - Tiered set-associative cache with CLOCK eviction

**Performance impact ranking:** 1 = highest expected impact, 10 = lowest.

---

## 1. [x] (Rank 1) Add instrumentation to identify actual bottlenecks â€” COMPLETED

**Goal:** Before optimizing, measure where time actually goes. Current metrics exist but aren't aggregated for decision-making.

**Why this matters:** The remaining optimizations are based on assumptions about where time is spent. Instrumentation will validate or invalidate these assumptions and guide priority.

**Implementation:**

1. **Add timing breakdown to `PackExecReport`:**
   ```rust
   // In pack_exec.rs, add to PackExecStats:
   pub decode_nanos: u64,           // Time in inflate/delta-apply
   pub cache_lookup_nanos: u64,     // Time in cache.get()
   pub fallback_resolve_nanos: u64, // Time in resolve_fallback_base()
   pub sink_emit_nanos: u64,        // Time in sink.emit()
   ```

2. **Add cache efficiency metrics:**
   ```rust
   // Compute and log after pack execution:
   let cache_hit_rate = stats.cache_hits as f64
       / (stats.cache_hits + stats.cache_misses) as f64;
   let fallback_rate = stats.fallback_base_decodes as f64
       / stats.decoded_offsets as f64;
   ```

3. **Add per-pack timing to `GitScanReport`:**
   - Track wall-clock time per pack in sharded vs non-sharded mode
   - Log which execution path was chosen (pack-parallel vs intra-pack sharded)

**Change points:**
- `src/git_scan/pack_exec.rs`: Add timing fields to `PackExecStats`, wrap hot-path sections with `Instant::now()` measurements
- `src/git_scan/runner.rs`: Aggregate timing across workers, log execution strategy decisions
- `src/bin/git_scan.rs`: Add `--perf-breakdown` flag to emit detailed timing

**Acceptance criteria:**
- Running `git_scan --perf-breakdown <repo>` prints:
  ```
  pack_exec breakdown:
    decode: 45.2% (1.23s)
    cache_lookup: 12.1% (0.33s)
    fallback_resolve: 8.4% (0.23s)
    sink_emit: 34.3% (0.94s)
  cache efficiency:
    hit_rate: 78.3%
    fallback_rate: 4.2%
  ```

**Estimated complexity:** Low (additive instrumentation, no behavioral changes)

**Implementation completed:** 2025-02-05
- Added `cache_lookup_nanos`, `fallback_resolve_nanos`, `sink_emit_nanos` to `PackExecStats`
- Instrumented cache lookups, fallback resolution, and sink emit calls
- Added `--perf-breakdown` flag to `git_scan` CLI
- All timing is feature-gated behind `git-perf` (zero overhead when disabled)
- See "Baseline Performance Results" section above for findings

---

## 2. [x] (Rank 2) Fix cache instrumentation and clean up vestigial admission policy

**Goal:** Properly measure cache effectiveness and remove dead code.

**Root cause:** Investigation revealed two issues:
1. **Measurement bug**: The `cache_hits`/`cache_misses` counters measured top-level blob lookups (always 0% because each blob is processed exactly once), NOT delta base lookups (where caching actually matters)
2. **Vestigial code**: The `should_cache_at_index()` admission policy was restrictive but had no effect since the cache was already workingâ€”we just weren't measuring it correctly

**Implementation (2025-02-05):**

1. **Added proper instrumentation** for delta base cache lookups:
```rust
// In EntryKind::OfsDelta handler:
let base = match cache.get(base_offset) {
    Some(base) => {
        report.stats.base_cache_hits += 1;  // NEW: Count actual useful cache hits
        BaseBytes { ... }
    }
    None => {
        report.stats.base_cache_misses += 1;  // NEW: Count actual useful cache misses
        decode_base_from_pack(...)
    }
}
```

2. **Removed vestigial code**:
   - Deleted `should_cache_at_index()` and `should_cache_for_offset()` functions (always returned `true`)
   - Removed `base_ref_counts` computation and parameter passing
   - Removed `cache_hits`, `cache_misses`, `cache_admission_skips` counters (measuring wrong thing)

**Actual results:**
- Base cache hit rate: **57-71%** across all tested repos
- Cache is working effectively and saving significant decode work

**Change points:**
- `src/git_scan/pack_exec.rs` - Added `base_cache_hits`/`base_cache_misses` counters, removed vestigial code
- `src/bin/git_scan.rs` - Updated `--perf-breakdown` output to show `base_cache_hit_rate`

---

## 2.5 [x] (Rank 2.5) Profile and optimize sink_emit (engine scan path) â€” COMPLETED

**Goal:** Identify and address bottlenecks in the `sink.emit()` path, which dominates pack execution time.

**Why this matters:** Baseline instrumentation (2025-02-05) revealed that `sink_emit` consumes 575-1040% of wall-clock time across all 7 tested workloads:

| Repository | sink_emit % | sink_emit time |
|------------|-------------|----------------|
| gitleaks   | 749.8%      | 1.09s          |
| go-git     | 654.6%      | 0.68s          |
| rocksdb    | 1040.8%     | 24.14s         |

This is the engine scanning phase (pattern matching, anchor validation, transform decoding). It was not originally identified as a bottleneck but dominates all other pack execution operations.

### Sub-Stage Instrumentation Results (2025-02-05)

Added 9 sub-stage counters behind the `git-perf` feature flag to break down what happens inside `sink_emit`. Results:

#### Scan Sub-Stage Breakdown

| Repository | vs_prefilter | validate | transform | reset | sort_dedup | **other** |
|------------|-------------|----------|-----------|-------|------------|-----------|
| gitleaks   | 21.7% (0.272s) | 44.8% (0.561s) | 3.9% (0.048s) | 0.0% | 0.0% | **29.6% (0.371s)** |
| go-git     | 17.9% (0.114s) | 1.4% (0.009s) | 1.1% (0.007s) | 0.1% | 0.0% | **79.6% (0.508s)** |
| rocksdb    | 23.3% (6.040s) | 1.1% (0.297s) | 0.5% (0.132s) | 0.0% | 0.0% | **75.0% (19.418s)** |

#### Scan Stats

| Repository | blobs  | chunks | zero_hit_chunks (%) | findings |
|------------|--------|--------|---------------------|----------|
| gitleaks   | 5,924  | 5,924  | 3,924 (66.2%)       | 9,449    |
| go-git     | 9,550  | 9,550  | 9,041 (94.7%)       | 80       |
| rocksdb    | 65,271 | 65,336 | 46,949 (71.9%)      | 71       |

#### Full `--perf-breakdown` Output

**gitleaks:**
```
pack_exec breakdown:
  decode: 167.4% (0.342s)
  cache_lookup: 0.2% (0.000s)
  fallback_resolve: 77.9% (0.159s)
  sink_emit: 613.9% (1.255s)
cache efficiency:
  base_cache_hit_rate: 70.9% (2933/4137)
  fallback_rate: 14.5%
scan breakdown (within sink_emit):
  vs_prefilter:  21.7% (0.272s)
  validate:      44.8% (0.561s)
  transform:     3.9% (0.048s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.0% (0.000s)
  other:         29.6% (0.371s)
scan stats:
  blobs: 5924  chunks: 5924  zero_hit_chunks: 3924 (66.2%)  findings: 9449
```

**go-git:**
```
pack_exec breakdown:
  decode: 304.7% (0.340s)
  cache_lookup: 0.3% (0.000s)
  fallback_resolve: 186.8% (0.208s)
  sink_emit: 575.0% (0.641s)
cache efficiency:
  base_cache_hit_rate: 67.0% (5489/8189)
  fallback_rate: 18.0%
scan breakdown (within sink_emit):
  vs_prefilter:  17.9% (0.114s)
  validate:      1.4% (0.009s)
  transform:     1.1% (0.007s)
  reset:         0.1% (0.000s)
  sort_dedup:    0.0% (0.000s)
  other:         79.6% (0.508s)
scan stats:
  blobs: 9550  chunks: 9550  zero_hit_chunks: 9041 (94.7%)  findings: 80
```

**rocksdb:**
```
pack_exec breakdown:
  decode: 512.0% (13.209s)
  cache_lookup: 0.1% (0.004s)
  fallback_resolve: 352.6% (9.096s)
  sink_emit: 1004.7% (25.922s)
cache efficiency:
  base_cache_hit_rate: 56.7% (33758/59512)
  fallback_rate: 22.8%
scan breakdown (within sink_emit):
  vs_prefilter:  23.3% (6.040s)
  validate:      1.1% (0.297s)
  transform:     0.5% (0.132s)
  reset:         0.0% (0.009s)
  sort_dedup:    0.0% (0.002s)
  other:         75.0% (19.418s)
scan stats:
  blobs: 65271  chunks: 65336  zero_hit_chunks: 46949 (71.9%)  findings: 71
```

### Extended Workload Comparison (2025-02-05)

Added 4 larger repositories to validate findings at scale.

#### Pack Exec Breakdown (Extended)

| Repository  | Blobs   | decode           | cache_lookup   | fallback_resolve  | sink_emit           | base_cache_hit_rate         | fallback_rate |
|-------------|---------|------------------|----------------|-------------------|---------------------|-----------------------------|---------------|
| react       | 107,595 | 518.8% (10.43s)  | 0.3% (0.01s)   | 479.9% (9.65s)    | 633.7% (12.74s)     | 66.6% (63309/95028)         | 7.4%          |
| node        | 495,059 | 582.0% (73.87s)  | 0.4% (0.05s)   | 420.7% (53.40s)   | 843.1% (107.02s)    | 51.1% (201298/393956)       | 20.6%         |
| vscode      | 444,946 | 651.2% (55.38s)  | 0.2% (0.02s)   | 388.8% (33.06s)   | 853.6% (72.58s)     | 49.0% (152775/311479)       | 20.8%         |
| tensorflow  | 694,540 | 575.2% (113.08s) | 0.2% (0.04s)   | 459.0% (90.25s)   | 773.3% (152.03s)    | 49.5% (303642/613300)       | 18.6%         |

#### Scan Sub-Stage Breakdown (Extended)

| Repository  | vs_prefilter       | validate          | transform       | reset        | sort_dedup   | **other**            |
|-------------|-------------------|-------------------|-----------------|--------------|--------------|----------------------|
| react       | 22.5% (2.86s)     | 5.4% (0.68s)      | 0.6% (0.07s)   | 0.0% (0.01s) | 0.0% (0.00s) | **71.5% (9.07s)**   |
| node        | 25.7% (27.47s)    | 2.2% (2.33s)      | 2.7% (2.85s)   | 0.1% (0.07s) | 0.0% (0.03s) | **69.3% (74.02s)**  |
| vscode      | 27.2% (19.69s)    | 19.8% (14.32s)    | 1.4% (0.99s)   | 0.1% (0.05s) | 0.0% (0.02s) | **51.5% (37.29s)**  |
| tensorflow  | 23.3% (35.35s)    | 1.0% (1.54s)      | 0.6% (0.98s)   | 0.0% (0.05s) | 0.0% (0.02s) | **75.0% (113.74s)** |

#### Scan Stats (Extended)

| Repository  | blobs    | chunks   | zero_hit_chunks (%)    | findings  |
|-------------|----------|----------|------------------------|-----------|
| react       | 107,595  | 107,604  | 99,315 (92.3%)         | 3,070     |
| node        | 495,059  | 496,516  | 452,745 (91.2%)        | 11,464    |
| vscode      | 444,946  | 445,209  | 278,152 (62.5%)        | 98,568    |
| tensorflow  | 694,540  | 695,819  | 620,540 (89.2%)        | 14,604    |

#### Full `--perf-breakdown` Output (Extended)

**react:**
```
pack_exec breakdown:
  decode: 518.8% (10.432s)
  cache_lookup: 0.3% (0.006s)
  fallback_resolve: 479.9% (9.648s)
  sink_emit: 633.7% (12.741s)
cache efficiency:
  base_cache_hit_rate: 66.6% (63309/95028)
  fallback_rate: 7.4%
scan breakdown (within sink_emit):
  vs_prefilter:  22.5% (2.858s)
  validate:      5.4% (0.684s)
  transform:     0.6% (0.073s)
  reset:         0.0% (0.006s)
  sort_dedup:    0.0% (0.002s)
  other:         71.5% (9.070s)
scan stats:
  blobs: 107595  chunks: 107604  zero_hit_chunks: 99315 (92.3%)  findings: 3070
```

**node:**
```
pack_exec breakdown:
  decode: 582.0% (73.870s)
  cache_lookup: 0.4% (0.045s)
  fallback_resolve: 420.7% (53.403s)
  sink_emit: 843.1% (107.018s)
cache efficiency:
  base_cache_hit_rate: 51.1% (201298/393956)
  fallback_rate: 20.6%
scan breakdown (within sink_emit):
  vs_prefilter:  25.7% (27.467s)
  validate:      2.2% (2.334s)
  transform:     2.7% (2.849s)
  reset:         0.1% (0.065s)
  sort_dedup:    0.0% (0.025s)
  other:         69.3% (74.019s)
scan stats:
  blobs: 495059  chunks: 496516  zero_hit_chunks: 452745 (91.2%)  findings: 11464
```

**vscode:**
```
pack_exec breakdown:
  decode: 651.2% (55.376s)
  cache_lookup: 0.2% (0.021s)
  fallback_resolve: 388.8% (33.064s)
  sink_emit: 853.6% (72.582s)
cache efficiency:
  base_cache_hit_rate: 49.0% (152775/311479)
  fallback_rate: 20.8%
scan breakdown (within sink_emit):
  vs_prefilter:  27.2% (19.688s)
  validate:      19.8% (14.316s)
  transform:     1.4% (0.988s)
  reset:         0.1% (0.052s)
  sort_dedup:    0.0% (0.015s)
  other:         51.5% (37.289s)
scan stats:
  blobs: 444946  chunks: 445209  zero_hit_chunks: 278152 (62.5%)  findings: 98568
```

**tensorflow:**
```
pack_exec breakdown:
  decode: 575.2% (113.075s)
  cache_lookup: 0.2% (0.044s)
  fallback_resolve: 459.0% (90.245s)
  sink_emit: 773.3% (152.026s)
cache efficiency:
  base_cache_hit_rate: 49.5% (303642/613300)
  fallback_rate: 18.6%
scan breakdown (within sink_emit):
  vs_prefilter:  23.3% (35.348s)
  validate:      1.0% (1.544s)
  transform:     0.6% (0.980s)
  reset:         0.0% (0.049s)
  sort_dedup:    0.0% (0.021s)
  other:         75.0% (113.740s)
scan stats:
  blobs: 694540  chunks: 695819  zero_hit_chunks: 620540 (89.2%)  findings: 14604
```

### Key Findings

*Updated with extended dataset (7 repos: gitleaks, go-git, rocksdb, react, node, vscode, tensorflow).*

1. **"other" dominates consistently (51-80%).** Across all 7 repos, "other" (RingChunker memcpy + scan_chunk_into orchestration) is the largest single component of scan time. It includes:
   - `RingChunker` `feed()`/`flush()` memcpy (copying blob bytes into the internal ring buffer)
   - `scan_chunk_into()` orchestration (work-queue loop dispatch, `drop_prefix_findings`, finding extraction in `scan_chunk`)
   - The gap between entering `scan_chunk_into` and entering `scan_rules_on_buffer` (scratch field setup, work-queue pop loop)
   - **Low-findings repos:** 69-80% (go-git, rocksdb, node, react, tensorflow)
   - **High-findings repos:** 30-52% (gitleaks, vscode) â€” validate takes a larger share

2. **Vectorscan prefilter is ~18-27% of scan time** across all 7 repos. This is the inherent O(blob_bytes Ã— pattern_complexity) cost of the DFA traversal â€” reducible only by changing the pattern set.

3. **validate scales with finding density, not blob count.**
   - gitleaks: 9,449 findings from 5,924 blobs â†’ validate is 44.8%
   - vscode: 98,568 findings from 444,946 blobs â†’ validate is 19.8%
   - tensorflow: 14,604 findings from 694,540 blobs â†’ validate is 1.0%
   - go-git/rocksdb with 71-80 findings â†’ validate is 1-1.4%
   This confirms validation cost is proportional to `findings / blobs`, not `blobs`.

4. **transform, reset, sort_dedup are negligible** (<4% each across all 7 repos). Not worth optimizing.

5. **62-95% of chunks produce zero prefilter hits.** Even vscode, with the highest finding density (98,568 findings), has 62.5% zero-hit chunks. The engine does significant work per chunk even when nothing matches â€” this is the "other" cost. For zero-hit chunks, the entire `scan_chunk_into` call is overhead: reset scratch, run Vectorscan (which returns empty), tear down.

6. **chunks â‰ˆ blobs** in all 7 repos â€” chunks are within 0.2% of blob count universally. Nearly all blobs fit in a single chunk (< 1 MiB). The RingChunker is doing one memcpy per blob with no chunking benefit. This confirms the RingChunker bypass optimization applies universally.

7. **Cache hit rate degrades for larger repos (~49-51%).**
   - Small repos (gitleaks, go-git): 67-71% hit rate
   - Medium repos (react): 67% hit rate
   - Large repos (node, vscode, tensorflow): 49-51% hit rate
   The fixed-size cache becomes less effective as working set grows. This validates item 3 (cache under-provisioning) as still relevant for large repos.

8. **fallback_resolve is significant on large repos.**
   - tensorflow: 90.25s (459% of wall-clock)
   - node: 53.40s (421%)
   - vscode: 33.06s (389%)
   Combined with lower cache hit rates (49-51%), this suggests items 3/6 (cache sizing + external base prefetch) become more impactful at scale.

### Optimization Candidates (Ranked by Data)

1. ~~**RingChunker bypass for small blobs**~~ â€” âœ… DONE (Item 2.5a). See results below.

2. ~~**Skip redundant capacity revalidation in reset_for_scan**~~ â€” âœ… DONE (Item 2.5b). See results below.

3. ~~**Binary blob detection**~~ â€” âœ… DONE (Item 2.5c). See results below.

4. ~~**Fast-path for zero-hit chunks**~~ â€” âœ… DONE (Item 2.5d). See results below.

### Implementation Details

**Counters added (behind `git-perf` feature flag):**

| Counter | Location | Measures |
|---------|----------|----------|
| `SCAN_VS_PREFILTER_NANOS` | `buffer_scan.rs` around `vs.scan_raw()` | Vectorscan DFA traversal |
| `SCAN_VALIDATE_NANOS` | `buffer_scan.rs` around touched_pairs loop | Window sort/merge + regex |
| `SCAN_TRANSFORM_NANOS` | `core.rs` around DecodeSpan work items | Base64/URL decode + rescan |
| `SCAN_SORT_DEDUP_NANOS` | `engine_adapter.rs` around `sort_unstable()+dedup()` | Per-blob findings postprocessing |
| `SCAN_RESET_NANOS` | `core.rs` around `scratch.reset_for_scan()` | Per-chunk scratch reset |
| `SCAN_BLOB_COUNT` | `engine_adapter.rs` on entry to `scan_blob_chunked_with_chunker` | Total blobs scanned |
| `SCAN_CHUNK_COUNT` | `engine_adapter.rs` on entry to `scan_chunk` | Total chunks processed |
| `SCAN_ZERO_HIT_CHUNKS` | `buffer_scan.rs` when `touched_pairs.is_empty()` | Chunks with no prefilter hits |
| `SCAN_FINDINGS_COUNT` | `scratch.rs` in `push_finding_with_drop_hint` | Total findings produced |

**Files modified:**
- `src/git_scan/perf.rs` â€” 9 new AtomicU64 statics, fields, record functions, snapshot/reset
- `src/engine/buffer_scan.rs` â€” vs_prefilter timing, validate timing, zero-hit counting
- `src/engine/core.rs` â€” reset timing, transform timing
- `src/git_scan/engine_adapter.rs` â€” blob/chunk counting, sort_dedup timing (+ empty guard)
- `src/engine/scratch.rs` â€” finding counting in `push_finding_with_drop_hint`
- `src/bin/git_scan.rs` â€” extended `--perf-breakdown` output with scan breakdown section

### Item 2.5a/2.5b/2.5c Implementation and Results

**Date:** 2025-02-05

**Changes implemented:**

1. **Item 2.5a â€” RingChunker bypass for small blobs:** Added fast path at the top of `scan_blob_chunked_with_chunker()`. When `blob.len() <= chunk_bytes`, the blob is scanned directly as a single `ChunkView` without copying into the ring buffer. All invariants preserved (AllocGuard, perf counters, sort/dedup).

2. **Item 2.5b â€” Skip redundant capacity revalidation in `reset_for_scan()`:** Added `capacity_validated: bool` field to `ScanScratch`. After the first `reset_for_scan()` completes VS-scratch pointer checks and capacity growth checks, subsequent calls skip the entire validation block (early return after per-scan state clears and accumulator resets).

3. **Item 2.5c â€” Binary blob detection:** Added `is_likely_binary()` using `memchr::memchr(0, ...)` to detect NUL bytes in the first 8192 bytes (matching Git's `buffer_is_binary` heuristic). Called at the top of `scan_blob_into_buf()`, short-circuiting with empty findings for binary blobs.

**New perf counters:** `scan_chunker_bypass_count`, `scan_binary_skip_count`.

**Files modified:**
- `src/git_scan/engine_adapter.rs` â€” RingChunker bypass fast path (2.5a), `is_likely_binary()` function + call (2.5c), tests
- `src/git_scan/perf.rs` â€” Two new counters: `scan_chunker_bypass_count`, `scan_binary_skip_count` (fields, statics, record fns, snapshot, reset)
- `src/bin/git_scan.rs` â€” Display new counters in `--perf-breakdown` output
- `src/engine/scratch.rs` â€” `capacity_validated` field + guard in `reset_for_scan()` (2.5b)

#### Post-Optimization Results

##### Scan Sub-Stage Breakdown (Post-2.5a/2.5b/2.5c)

| Repository  | vs_prefilter       | validate          | transform       | reset        | sort_dedup   | **other**            |
|-------------|-------------------|-------------------|-----------------|--------------|--------------|----------------------|
| gitleaks    | 23.0% (0.249s)    | 39.0% (0.422s)    | 4.7% (0.051s)  | 0.0% (0.000s)| 0.0% (0.000s)| **33.3% (0.360s)**  |
| go-git      | 21.9% (0.119s)    | 2.1% (0.011s)     | 0.6% (0.003s)  | 0.0% (0.000s)| 0.0% (0.000s)| **75.2% (0.409s)**  |
| rocksdb     | 23.8% (5.544s)    | 1.4% (0.316s)     | 0.7% (0.174s)  | 0.0% (0.009s)| 0.0% (0.002s)| **74.0% (17.238s)** |
| react       | 25.1% (2.562s)    | 5.3% (0.536s)     | 0.5% (0.053s)  | 0.3% (0.028s)| 0.0% (0.002s)| **68.8% (7.024s)**  |
| node        | 26.6% (27.039s)   | 2.0% (1.987s)     | 2.6% (2.616s)  | 0.1% (0.069s)| 0.0% (0.014s)| **68.8% (69.900s)** |
| vscode      | 28.0% (20.716s)   | 20.9% (15.507s)   | 1.3% (0.990s)  | 0.0% (0.029s)| 0.0% (0.017s)| **49.7% (36.810s)** |
| tensorflow  | 23.8% (35.686s)   | 1.1% (1.626s)     | 0.8% (1.134s)  | 0.0% (0.027s)| 0.0% (0.014s)| **74.3% (111.319s)**|

##### Scan Stats (Post-2.5a/2.5b/2.5c)

| Repository  | blobs    | chunks   | zero_hit_chunks (%)    | findings  | chunker_bypass (%) | binary_skip |
|-------------|----------|----------|------------------------|-----------|--------------------|-------------|
| gitleaks    | 5,747    | 5,748    | 3,749 (65.2%)          | 9,446     | 5,746 (100.0%)     | 177         |
| go-git      | 9,548    | 9,548    | 9,039 (94.7%)          | 80        | 9,548 (100.0%)     | 2           |
| rocksdb     | 65,131   | 65,134   | 46,747 (71.8%)         | 71        | 65,128 (100.0%)    | 140         |
| react       | 107,232  | 107,232  | 98,943 (92.3%)         | 3,070     | 107,232 (100.0%)   | 363         |
| node        | 493,851  | 494,954  | 451,217 (91.2%)        | 11,464    | 493,220 (99.9%)    | 1,208       |
| vscode      | 443,973  | 444,226  | 277,207 (62.4%)        | 98,568    | 443,768 (100.0%)   | 973         |
| tensorflow  | 693,490  | 694,694  | 619,416 (89.2%)        | 14,604    | 692,351 (99.8%)    | 1,050       |

##### Before/After Comparison (scan sub-stage: "other")

| Repository  | Before (other %)  | Before (other s) | After (other %) | After (other s) | Delta (s)  | Delta (%) |
|-------------|-------------------|-------------------|-----------------|-----------------|------------|-----------|
| gitleaks    | 29.6%             | 0.371s            | 33.3%           | 0.360s          | -0.011s    | -3.0%     |
| go-git      | 79.6%             | 0.508s            | 75.2%           | 0.409s          | -0.099s    | -19.5%    |
| rocksdb     | 75.0%             | 19.418s           | 74.0%           | 17.238s         | -2.180s    | -11.2%    |
| react       | 71.5%             | 9.070s            | 68.8%           | 7.024s          | -2.046s    | -22.6%    |
| node        | 69.3%             | 74.019s           | 68.8%           | 69.900s         | -4.119s    | -5.6%     |
| vscode      | 51.5%             | 37.289s           | 49.7%           | 36.810s         | -0.479s    | -1.3%     |
| tensorflow  | 75.0%             | 113.740s          | 74.3%           | 111.319s        | -2.421s    | -2.1%     |

##### Full `--perf-breakdown` Output (Post-2.5a/2.5b/2.5c)

**gitleaks:**
```
pack_exec breakdown:
  decode: 183.8% (0.242s)
  cache_lookup: 0.2% (0.000s)
  fallback_resolve: 85.7% (0.113s)
  sink_emit: 829.7% (1.093s)
cache efficiency:
  base_cache_hit_rate: 70.9% (2933/4137)
  fallback_rate: 14.5%
scan breakdown (within sink_emit):
  vs_prefilter:  23.0% (0.249s)
  validate:      39.0% (0.422s)
  transform:     4.7% (0.051s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.0% (0.000s)
  other:         33.3% (0.360s)
scan stats:
  blobs: 5747  chunks: 5748  zero_hit_chunks: 3749 (65.2%)  findings: 9446
  chunker_bypass: 5746 (100.0%)  binary_skip: 177
```

**go-git:**
```
pack_exec breakdown:
  decode: 289.7% (0.293s)
  cache_lookup: 0.3% (0.000s)
  fallback_resolve: 178.3% (0.180s)
  sink_emit: 540.5% (0.547s)
cache efficiency:
  base_cache_hit_rate: 67.0% (5489/8189)
  fallback_rate: 18.0%
scan breakdown (within sink_emit):
  vs_prefilter:  21.9% (0.119s)
  validate:      2.1% (0.011s)
  transform:     0.6% (0.003s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.0% (0.000s)
  other:         75.2% (0.409s)
scan stats:
  blobs: 9548  chunks: 9548  zero_hit_chunks: 9039 (94.7%)  findings: 80
  chunker_bypass: 9548 (100.0%)  binary_skip: 2
```

**rocksdb:**
```
pack_exec breakdown:
  decode: 546.3% (12.751s)
  cache_lookup: 0.3% (0.007s)
  fallback_resolve: 365.7% (8.536s)
  sink_emit: 999.4% (23.326s)
cache efficiency:
  base_cache_hit_rate: 56.7% (33758/59512)
  fallback_rate: 22.8%
scan breakdown (within sink_emit):
  vs_prefilter:  23.8% (5.544s)
  validate:      1.4% (0.316s)
  transform:     0.7% (0.174s)
  reset:         0.0% (0.009s)
  sort_dedup:    0.0% (0.002s)
  other:         74.0% (17.238s)
scan stats:
  blobs: 65131  chunks: 65134  zero_hit_chunks: 46747 (71.8%)  findings: 71
  chunker_bypass: 65128 (100.0%)  binary_skip: 140
```

**react:**
```
pack_exec breakdown:
  decode: 532.6% (9.115s)
  cache_lookup: 0.2% (0.004s)
  fallback_resolve: 463.4% (7.930s)
  sink_emit: 602.6% (10.311s)
cache efficiency:
  base_cache_hit_rate: 66.6% (63309/95028)
  fallback_rate: 7.4%
scan breakdown (within sink_emit):
  vs_prefilter:  25.1% (2.562s)
  validate:      5.3% (0.536s)
  transform:     0.5% (0.053s)
  reset:         0.3% (0.028s)
  sort_dedup:    0.0% (0.002s)
  other:         68.8% (7.024s)
scan stats:
  blobs: 107232  chunks: 107232  zero_hit_chunks: 98943 (92.3%)  findings: 3070
  chunker_bypass: 107232 (100.0%)  binary_skip: 363
```

**node:**
```
pack_exec breakdown:
  decode: 633.2% (79.628s)
  cache_lookup: 0.5% (0.067s)
  fallback_resolve: 463.2% (58.249s)
  sink_emit: 811.6% (102.066s)
cache efficiency:
  base_cache_hit_rate: 51.1% (201298/393956)
  fallback_rate: 20.6%
scan breakdown (within sink_emit):
  vs_prefilter:  26.6% (27.039s)
  validate:      2.0% (1.987s)
  transform:     2.6% (2.616s)
  reset:         0.1% (0.069s)
  sort_dedup:    0.0% (0.014s)
  other:         68.8% (69.900s)
scan stats:
  blobs: 493851  chunks: 494954  zero_hit_chunks: 451217 (91.2%)  findings: 11464
  chunker_bypass: 493220 (99.9%)  binary_skip: 1208
```

**vscode:**
```
pack_exec breakdown:
  decode: 659.9% (57.564s)
  cache_lookup: 0.5% (0.042s)
  fallback_resolve: 397.6% (34.682s)
  sink_emit: 853.8% (74.477s)
cache efficiency:
  base_cache_hit_rate: 49.0% (152775/311479)
  fallback_rate: 20.8%
scan breakdown (within sink_emit):
  vs_prefilter:  28.0% (20.716s)
  validate:      20.9% (15.507s)
  transform:     1.3% (0.990s)
  reset:         0.0% (0.029s)
  sort_dedup:    0.0% (0.017s)
  other:         49.7% (36.810s)
scan stats:
  blobs: 443973  chunks: 444226  zero_hit_chunks: 277207 (62.4%)  findings: 98568
  chunker_bypass: 443768 (100.0%)  binary_skip: 973
```

**tensorflow:**
```
pack_exec breakdown:
  decode: 592.3% (118.497s)
  cache_lookup: 0.3% (0.070s)
  fallback_resolve: 470.2% (94.083s)
  sink_emit: 752.2% (150.505s)
cache efficiency:
  base_cache_hit_rate: 49.5% (303642/613300)
  fallback_rate: 18.6%
scan breakdown (within sink_emit):
  vs_prefilter:  23.8% (35.686s)
  validate:      1.1% (1.626s)
  transform:     0.8% (1.134s)
  reset:         0.0% (0.027s)
  sort_dedup:    0.0% (0.014s)
  other:         74.3% (111.319s)
scan stats:
  blobs: 693490  chunks: 694694  zero_hit_chunks: 619416 (89.2%)  findings: 14604
  chunker_bypass: 692351 (99.8%)  binary_skip: 1050
```

#### Key Findings from Items 2.5a/2.5b/2.5c

1. **Chunker bypass is universal** â€” 99.8-100.0% of blobs take the fast path across all 7 repos, confirming that virtually all blobs fit in a single chunk. The memcpy through the ring buffer was unnecessary for the overwhelming majority of blobs.

2. **Binary skip is modest** â€” 2-1,208 blobs skipped per repo. The largest repos (node, tensorflow) skip ~1,000 binary blobs, which is a small fraction of total blobs (<0.2%). The benefit is more about avoiding wasted scan work on those blobs than reducing total count.

3. **"Other" reduced 1-23% in absolute time** â€” The most impactful repos were react (-22.6%, -2.0s) and go-git (-19.5%, -0.1s). Large repos see smaller relative improvement (node -5.6%, tensorflow -2.1%) because the remaining "other" cost is dominated by `scan_chunk_into` orchestration (work-queue loop, Vectorscan scratch setup, finding extraction), not the memcpy.

4. **"Other" still dominates (50-75%)** â€” The RingChunker bypass eliminated the memcpy overhead but did not close the gap. The remaining "other" cost is the per-chunk `scan_chunk_into` orchestration: scratch reset, Vectorscan stream open/close, work-queue dispatch, finding extraction. This is intrinsic per-chunk overhead.

5. **`reset_for_scan` overhead reduced to ~0.0%** â€” Item 2.5b successfully eliminated the capacity revalidation cost. The `reset` line shows 0.0% across all repos (previously already low, now negligible).

6. **Next bottleneck: per-chunk orchestration overhead** â€” With the memcpy eliminated, the remaining "other" is dominated by the per-chunk cost of entering and exiting `scan_chunk_into`. For repos with 600K+ zero-hit chunks (tensorflow: 619K), this adds up to >100s of intrinsic overhead. Further optimization requires either (a) reducing per-chunk cost in the engine itself or (b) skipping chunks entirely when possible.

### Item 2.5d â€” Zero-Hit Chunk Prefilter Bypass

**Date:** 2026-02-05

**Goal:** Eliminate per-chunk orchestration overhead for zero-hit chunks (62-95% of all chunks) by hoisting the Vectorscan prefilter above `reset_for_scan()` in `scan_chunk_into`. If zero hits, return immediately â€” skipping scratch reset, work-queue setup, and the entire orchestration layer.

**Changes implemented:**

1. **Hoist prefilter above reset** â€” Run Vectorscan `scan_raw()` directly in `scan_chunk_into` *before* `reset_for_scan()`. Only the prefilter accumulator state (`touched_pairs`, `hit_acc_pool`) and VS scratch are needed; everything else is deferred.

2. **Fast path for zero-hit chunks** â€” When `touched_pairs.is_empty()` after prefilter, clear only `out`/`norm_hash`/`drop_hint_end` (what callers read) and return immediately. Skips: scratch reset (~28 fields), work-queue push/dispatch, `scan_rules_on_buffer` entry, transform loop, finding extraction.

3. **Selective reset for hit path** â€” New `reset_for_scan_after_prefilter()` clears all scratch state *except* `hit_acc_pool`/`touched_pairs`, preserving prefilter results. One-shot flag `root_prefilter_done` signals `scan_rules_on_buffer` to skip its redundant prefilter run for the root buffer.

4. **Extract `ensure_capacity()`** â€” Idempotent capacity validation extracted from `reset_for_scan` into its own method, called once at the top of `scan_chunk_into`.

**New perf counter:** `scan_prefilter_bypass_count` â€” tracks how often the fast path fires.

**Files modified:**
- `src/engine/scratch.rs` â€” `root_prefilter_done`, `root_prefilter_saw_utf16` fields; `ensure_capacity()` extraction; `reset_for_scan_after_prefilter()` method
- `src/engine/core.rs` â€” Restructured `scan_chunk_into` preamble with prefilter-first gate (Steps A-F)
- `src/engine/buffer_scan.rs` â€” Skip-prefilter gate consuming one-shot flag; removed dead `used_vectorscan` variable
- `src/git_scan/perf.rs` â€” `scan_prefilter_bypass_count` counter (static, field, record fn, snapshot, reset)
- `src/bin/git_scan.rs` â€” Display prefilter bypass stats in `--perf-breakdown`

#### Post-Optimization Results (Post-2.5d)

##### Scan Sub-Stage Breakdown (Post-2.5d)

| Repository  | vs_prefilter       | validate          | transform       | reset        | sort_dedup   | **other**            |
|-------------|-------------------|-------------------|-----------------|--------------|--------------|----------------------|
| gitleaks    | 30.8% (0.231s)    | 45.4% (0.341s)    | 4.6% (0.035s)  | 0.0% (0.000s)| 0.0% (0.000s)| **19.2% (0.144s)**  |
| go-git      | 67.5% (0.114s)    | 9.9% (0.017s)     | 2.1% (0.004s)  | 0.0% (0.000s)| 0.1% (0.000s)| **20.4% (0.034s)**  |
| rocksdb     | 35.7% (5.375s)    | 2.0% (0.294s)     | 0.5% (0.076s)  | 0.1% (0.011s)| 0.0% (0.001s)| **61.8% (9.296s)**  |
| react       | 48.2% (2.534s)    | 11.1% (0.587s)    | 0.2% (0.011s)  | 0.0% (0.000s)| 0.1% (0.003s)| **40.4% (2.127s)**  |
| node        | 47.6% (25.838s)   | 4.0% (2.188s)     | 0.7% (0.391s)  | 0.0% (0.001s)| 0.0% (0.014s)| **47.6% (25.797s)** |
| vscode      | 35.1% (19.439s)   | 26.9% (14.904s)   | 0.5% (0.273s)  | 0.0% (0.005s)| 0.0% (0.018s)| **37.5% (20.773s)** |
| tensorflow  | 52.7% (36.602s)   | 2.6% (1.785s)     | 0.3% (0.228s)  | 0.0% (0.003s)| 0.0% (0.023s)| **44.4% (30.835s)** |

##### Scan Stats (Post-2.5d)

| Repository  | blobs    | chunks   | zero_hit_chunks (%)    | findings  | chunker_bypass (%) | binary_skip | prefilter_bypass (%) |
|-------------|----------|----------|------------------------|-----------|--------------------|-------------|----------------------|
| gitleaks    | 5,747    | 5,748    | 3,749 (65.2%)          | 9,446     | 5,746 (100.0%)     | 177         | 3,749 (65.2%)        |
| go-git      | 9,548    | 9,548    | 9,039 (94.7%)          | 80        | 9,548 (100.0%)     | 2           | 9,039 (94.7%)        |
| rocksdb     | 65,131   | 65,134   | 46,747 (71.8%)         | 71        | 65,128 (100.0%)    | 140         | 46,747 (71.8%)       |
| react       | 107,232  | 107,232  | 98,943 (92.3%)         | 3,070     | 107,232 (100.0%)   | 363         | 98,943 (92.3%)       |
| node        | 493,851  | 494,954  | 451,217 (91.2%)        | 11,464    | 493,220 (99.9%)    | 1,208       | 451,217 (91.2%)      |
| vscode      | 443,973  | 444,226  | 277,207 (62.4%)        | 98,565    | 443,768 (100.0%)   | 973         | 277,207 (62.4%)      |
| tensorflow  | 693,490  | 694,694  | 619,416 (89.2%)        | 14,604    | 692,351 (99.8%)    | 1,050       | 619,416 (89.2%)      |

##### Before/After Comparison: sink_emit total time

| Repository  | Before (sink_emit) | After (sink_emit) | Delta (s)   | Delta (%)  |
|-------------|--------------------|--------------------|-------------|------------|
| gitleaks    | 1.093s             | 0.753s             | **-0.340s** | **-31.1%** |
| go-git      | 0.547s             | 0.185s             | **-0.362s** | **-66.2%** |
| rocksdb     | 23.326s            | 15.096s            | **-8.230s** | **-35.3%** |
| react       | 10.311s            | 5.321s             | **-4.990s** | **-48.4%** |
| node        | 102.066s           | 54.579s            | **-47.487s**| **-46.5%** |
| vscode      | 74.477s            | 55.769s            | **-18.708s**| **-25.1%** |
| tensorflow  | 150.505s           | 69.962s            | **-80.543s**| **-53.5%** |

##### Before/After Comparison: scan sub-stage "other"

| Repository  | Before (other %)  | Before (other s) | After (other %) | After (other s) | Delta (s)    | Delta (%)  |
|-------------|-------------------|-------------------|-----------------|-----------------|--------------|------------|
| gitleaks    | 33.3%             | 0.360s            | 19.2%           | 0.144s          | **-0.216s**  | **-60.0%** |
| go-git      | 75.2%             | 0.409s            | 20.4%           | 0.034s          | **-0.375s**  | **-91.7%** |
| rocksdb     | 74.0%             | 17.238s           | 61.8%           | 9.296s          | **-7.942s**  | **-46.1%** |
| react       | 68.8%             | 7.024s            | 40.4%           | 2.127s          | **-4.897s**  | **-69.7%** |
| node        | 68.8%             | 69.900s           | 47.6%           | 25.797s         | **-44.103s** | **-63.1%** |
| vscode      | 49.7%             | 36.810s           | 37.5%           | 20.773s         | **-16.037s** | **-43.6%** |
| tensorflow  | 74.3%             | 111.319s          | 44.4%           | 30.835s         | **-80.484s** | **-72.3%** |

##### Full `--perf-breakdown` Output (Post-2.5d)

**gitleaks:**
```
pack_exec breakdown:
  decode: 176.5% (0.252s)
  cache_lookup: 0.2% (0.000s)
  fallback_resolve: 86.9% (0.124s)
  sink_emit: 527.4% (0.753s)
cache efficiency:
  base_cache_hit_rate: 70.9% (2933/4137)
  fallback_rate: 14.5%
scan breakdown (within sink_emit):
  vs_prefilter:  30.8% (0.231s)
  validate:      45.4% (0.341s)
  transform:     4.6% (0.035s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.0% (0.000s)
  other:         19.2% (0.144s)
scan stats:
  blobs: 5747  chunks: 5748  zero_hit_chunks: 3749 (65.2%)  findings: 9446
  chunker_bypass: 5746 (100.0%)  binary_skip: 177  prefilter_bypass: 3749 (65.2%)
```

**go-git:**
```
pack_exec breakdown:
  decode: 558.4% (0.353s)
  cache_lookup: 0.6% (0.000s)
  fallback_resolve: 337.1% (0.213s)
  sink_emit: 293.5% (0.185s)
cache efficiency:
  base_cache_hit_rate: 67.0% (5489/8189)
  fallback_rate: 18.0%
scan breakdown (within sink_emit):
  vs_prefilter:  67.5% (0.114s)
  validate:      9.9% (0.017s)
  transform:     2.1% (0.004s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.1% (0.000s)
  other:         20.4% (0.034s)
scan stats:
  blobs: 9548  chunks: 9548  zero_hit_chunks: 9039 (94.7%)  findings: 80
  chunker_bypass: 9548 (100.0%)  binary_skip: 2  prefilter_bypass: 9039 (94.7%)
```

**rocksdb:**
```
pack_exec breakdown:
  decode: 631.6% (11.876s)
  cache_lookup: 0.3% (0.006s)
  fallback_resolve: 420.7% (7.911s)
  sink_emit: 802.9% (15.096s)
cache efficiency:
  base_cache_hit_rate: 56.7% (33758/59512)
  fallback_rate: 22.8%
scan breakdown (within sink_emit):
  vs_prefilter:  35.7% (5.375s)
  validate:      2.0% (0.294s)
  transform:     0.5% (0.076s)
  reset:         0.1% (0.011s)
  sort_dedup:    0.0% (0.001s)
  other:         61.8% (9.296s)
scan stats:
  blobs: 65131  chunks: 65134  zero_hit_chunks: 46747 (71.8%)  findings: 71
  chunker_bypass: 65128 (100.0%)  binary_skip: 140  prefilter_bypass: 46747 (71.8%)
```

**react:**
```
pack_exec breakdown:
  decode: 650.7% (9.052s)
  cache_lookup: 0.4% (0.006s)
  fallback_resolve: 565.6% (7.869s)
  sink_emit: 382.4% (5.321s)
cache efficiency:
  base_cache_hit_rate: 66.6% (63309/95028)
  fallback_rate: 7.4%
scan breakdown (within sink_emit):
  vs_prefilter:  48.2% (2.534s)
  validate:      11.1% (0.587s)
  transform:     0.2% (0.011s)
  reset:         0.0% (0.000s)
  sort_dedup:    0.1% (0.003s)
  other:         40.4% (2.127s)
scan stats:
  blobs: 107232  chunks: 107232  zero_hit_chunks: 98943 (92.3%)  findings: 3070
  chunker_bypass: 107232 (100.0%)  binary_skip: 363  prefilter_bypass: 98943 (92.3%)
```

**node:**
```
pack_exec breakdown:
  decode: 814.2% (77.938s)
  cache_lookup: 0.5% (0.050s)
  fallback_resolve: 589.0% (56.388s)
  sink_emit: 570.1% (54.579s)
cache efficiency:
  base_cache_hit_rate: 51.1% (201298/393956)
  fallback_rate: 20.6%
scan breakdown (within sink_emit):
  vs_prefilter:  47.6% (25.838s)
  validate:      4.0% (2.188s)
  transform:     0.7% (0.391s)
  reset:         0.0% (0.001s)
  sort_dedup:    0.0% (0.014s)
  other:         47.6% (25.797s)
scan stats:
  blobs: 493851  chunks: 494954  zero_hit_chunks: 451217 (91.2%)  findings: 11464
  chunker_bypass: 493220 (99.9%)  binary_skip: 1208  prefilter_bypass: 451217 (91.2%)
```

**vscode:**
```
pack_exec breakdown:
  decode: 704.6% (54.065s)
  cache_lookup: 0.4% (0.029s)
  fallback_resolve: 416.5% (31.960s)
  sink_emit: 726.8% (55.769s)
cache efficiency:
  base_cache_hit_rate: 49.0% (152775/311479)
  fallback_rate: 20.8%
scan breakdown (within sink_emit):
  vs_prefilter:  35.1% (19.439s)
  validate:      26.9% (14.904s)
  transform:     0.5% (0.273s)
  reset:         0.0% (0.005s)
  sort_dedup:    0.0% (0.018s)
  other:         37.5% (20.773s)
scan stats:
  blobs: 443973  chunks: 444226  zero_hit_chunks: 277207 (62.4%)  findings: 98565
  chunker_bypass: 443768 (100.0%)  binary_skip: 973  prefilter_bypass: 277207 (62.4%)
```

**tensorflow:**
```
pack_exec breakdown:
  decode: 771.0% (113.965s)
  cache_lookup: 0.4% (0.055s)
  fallback_resolve: 616.2% (91.086s)
  sink_emit: 473.3% (69.962s)
cache efficiency:
  base_cache_hit_rate: 49.5% (303642/613300)
  fallback_rate: 18.6%
scan breakdown (within sink_emit):
  vs_prefilter:  52.7% (36.602s)
  validate:      2.6% (1.785s)
  transform:     0.3% (0.228s)
  reset:         0.0% (0.003s)
  sort_dedup:    0.0% (0.023s)
  other:         44.4% (30.835s)
scan stats:
  blobs: 693490  chunks: 694694  zero_hit_chunks: 619416 (89.2%)  findings: 14604
  chunker_bypass: 692351 (99.8%)  binary_skip: 1050  prefilter_bypass: 619416 (89.2%)
```

#### Key Findings from Item 2.5d

1. **Prefilter bypass fires at the same rate as zero-hit chunks** â€” 62-95% of chunks across all 7 repos. The `prefilter_bypass` counter exactly matches `zero_hit_chunks`, confirming the fast path triggers correctly for every zero-hit chunk.

2. **"Other" reduced 44-92% in absolute time** â€” The most impactful optimization yet:
   - **go-git:** -91.7% (0.409s â†’ 0.034s) â€” 94.7% zero-hit chunks, nearly all overhead eliminated
   - **tensorflow:** -72.3% (111.3s â†’ 30.8s) â€” 80.5s saved, by far the largest absolute reduction
   - **react:** -69.7% (7.0s â†’ 2.1s) â€” 92.3% zero-hit chunks
   - **node:** -63.1% (69.9s â†’ 25.8s) â€” 44.1s saved
   - **gitleaks:** -60.0% (0.36s â†’ 0.14s) â€” even with only 65% zero-hit chunks
   - **rocksdb:** -46.1% (17.2s â†’ 9.3s)
   - **vscode:** -43.6% (36.8s â†’ 20.8s) â€” lowest improvement due to lowest zero-hit rate (62.4%)

3. **sink_emit total time reduced 25-66%** â€” Because "other" was the dominant component of sink_emit, eliminating it has a massive compounding effect:
   - **go-git:** -66.2% (0.547s â†’ 0.185s)
   - **tensorflow:** -53.5% (150.5s â†’ 70.0s)
   - **react:** -48.4% (10.3s â†’ 5.3s)
   - **node:** -46.5% (102.1s â†’ 54.6s)
   - **rocksdb:** -35.3% (23.3s â†’ 15.1s)
   - **gitleaks:** -31.1% (1.09s â†’ 0.75s)
   - **vscode:** -25.1% (74.5s â†’ 55.8s)

4. **"Other" is no longer the dominant component** for most repos. Vectorscan prefilter is now the largest single component in 5 of 7 repos (go-git 67.5%, tensorflow 52.7%, react 48.2%, node 47.6%, rocksdb 35.7%). For gitleaks and vscode, validate dominates (45.4% and 26.9%) because they have high finding density.

5. **Improvement correlates with zero-hit chunk ratio** â€” go-git (94.7% zero-hit) sees the biggest relative "other" reduction (-91.7%). vscode (62.4% zero-hit) sees the smallest (-43.6%). This confirms the optimization works precisely as designed: the more zero-hit chunks, the more overhead is eliminated.

6. **No correctness regressions** â€” Finding counts are identical (gitleaks: 9,446, go-git: 80, rocksdb: 71, react: 3,070, node: 11,464, vscode: 98,565, tensorflow: 14,604). Blob/chunk counts unchanged. Cache hit rates unchanged. The optimization is purely about skipping unnecessary work on zero-hit chunks.

7. **Remaining "other" is now the residual per-hit-chunk orchestration** â€” For hit chunks (5-38% of total), the work-queue dispatch, finding extraction, and transform loop overhead still exists. This is expected and proportional to actual work done. Further reduction would require restructuring the hit-path pipeline itself.

### Cumulative Impact: Items 2.5a through 2.5d

| Repository  | Original "other" (s) | Post-2.5a/b/c (s) | Post-2.5d (s) | Total reduction | Total reduction (%) |
|-------------|----------------------|--------------------|---------------|-----------------|---------------------|
| gitleaks    | 0.371s               | 0.360s             | 0.144s        | -0.227s         | -61.2%              |
| go-git      | 0.508s               | 0.409s             | 0.034s        | -0.474s         | -93.3%              |
| rocksdb     | 19.418s              | 17.238s            | 9.296s        | -10.122s        | -52.1%              |
| react       | 9.070s               | 7.024s             | 2.127s        | -6.943s         | -76.5%              |
| node        | 74.019s              | 69.900s            | 25.797s       | -48.222s        | -65.1%              |
| vscode      | 37.289s              | 36.810s            | 20.773s       | -16.516s        | -44.3%              |
| tensorflow  | 113.740s             | 111.319s           | 30.835s       | -82.905s        | -72.9%              |

### Next Steps

- **Vectorscan prefilter is now the dominant cost** (35-68% of scan time) for low-finding-density repos. This is the inherent O(blob_bytes x pattern_complexity) DFA traversal â€” reducible only by optimizing the pattern set or reducing bytes scanned.
- **Validate path is the dominant cost** for high-finding-density repos (gitleaks 45.4%, vscode 26.9%). Regex optimization or finding deduplication could help here.
- Consider item 3 (cache under-provisioning) for large repos where cache hit rate is 49-51%
- Re-evaluate whether further scan-path optimization has diminishing returns vs addressing `fallback_resolve` (32-91s on large repos)

---

## 3. [ ] (Rank 3) Fix cache under-provisioning in sharded execution

**Goal:** Ensure each shard has adequate cache when running intra-pack parallel execution.

**Why this matters:** Currently, when sharding a single pack across N workers, the total cache budget is divided by N (see `runner.rs:1093-1095`), but each shard processes 1/N of the offsets with potential cross-shard delta dependencies. This means:
- Each shard has 1/N cache capacity
- But may need to decode bases that "belong" to other shards
- Results in cache thrashing and redundant `fallback_base_decodes`

**Current behavior (problematic):**
```rust
// runner.rs:1093-1095
let pack_cache_target =
    estimate_pack_cache_bytes(config.pack_cache_bytes, &pack_mmaps, &used_pack_ids);
// This divides total budget across packs, then each shard gets a slice
```

When sharding intra-pack at `runner.rs:1458`, each thread creates:
```rust
let mut scratch = PackExecScratch::default();
// scratch contains a fresh cache with default sizing, NOT the divided budget
```

**New behavior:**
1. For intra-pack sharding, **over-provision per-shard caches** by a factor of 2x
2. Rationale: cross-shard base dependencies mean each shard may need to cache entries that logically belong to other shards

**Implementation:**

1. **Compute shard-aware cache budget:**
   ```rust
   // In runner.rs, before spawning shard threads:
   let sharded_cache_bytes = if sharding_intra_pack {
       // Over-provision: each shard gets 2x its "fair share"
       // Capped at total configured budget
       (pack_cache_bytes / pack_exec_workers as u32)
           .saturating_mul(2)
           .min(pack_cache_bytes)
   } else {
       pack_cache_bytes
   };
   ```

2. **Pass cache budget to shard threads:**
   ```rust
   // Currently scratch is default-initialized; change to:
   let mut scratch = PackExecScratch::with_cache_bytes(sharded_cache_bytes);
   ```

3. **Add `PackExecScratch::with_cache_bytes()` constructor:**
   ```rust
   impl PackExecScratch {
       pub fn with_cache_bytes(cache_bytes: u32) -> Self {
           Self {
               cache: PackCache::new(cache_bytes),
               // ... other fields with defaults
           }
       }
   }
   ```

**Change points:**
- `src/git_scan/runner.rs:1421-1470` (sharded execution block)
- `src/git_scan/pack_exec.rs` (add `PackExecScratch::with_cache_bytes`)

**Metrics to validate:**
- Compare `fallback_base_decodes` before/after
- Compare `cache_hits` / `cache_misses` ratio before/after
- Wall-clock time for sharded execution on single large pack

**Estimated complexity:** Low-medium (localized changes, clear before/after measurement)

---

## 4. [x] (Rank 4) Refine pack-level vs intra-pack parallelism heuristic â€” COMPLETED

**Goal:** Choose the optimal execution strategy based on pack characteristics, not just pack count.

**Why this matters:** Pack-level parallelism preserves sequential access within each pack and avoids cross-shard cache thrashing. The original heuristic (now replaced) only considered pack count:

```rust
let prefer_pack_parallelism =
    pack_exec_workers > 1 && pack_plan_count >= pack_exec_workers;
```

This fails for:
- **One large pack with many candidates:** Should shard intra-pack, currently does
- **Many tiny packs with few candidates each:** Should consolidate, currently parallelizes
- **Mixed sizes:** Should assign workers proportionally to pack size

**Note:** The final implementation intentionally avoids `plan.stats` and derives strategy inputs directly from core `PackPlan` structure (see completion notes below).

**New behavior:**

1. **Compute per-pack "weight"** based on candidate count and pack size:
   ```rust
   struct PackWeight {
       pack_id: u16,
       candidates: u32,
       need_offsets: u32,
       span_bytes: u64,
   }

   fn compute_pack_weight(plan: &PackPlan) -> PackWeight {
       PackWeight {
           pack_id: plan.pack_id,
           candidates: plan.stats.candidate_count,
           need_offsets: plan.stats.need_count,
           span_bytes: plan.stats.candidate_span,
       }
   }
   ```

2. **Decision logic:**
   ```rust
   enum ExecStrategy {
       /// One worker per pack, no intra-pack sharding
       PackParallel,
       /// Single pack sharded across workers
       IntraPackSharded { pack_id: u16 },
       /// Mixed: large packs get multiple workers, small packs share workers
       Hybrid { assignments: Vec<(u16, usize)> }, // (pack_id, worker_count)
   }

   fn select_exec_strategy(
       weights: &[PackWeight],
       workers: usize,
   ) -> ExecStrategy {
       let total_work: u64 = weights.iter()
           .map(|w| w.need_offsets as u64)
           .sum();

       // If single pack dominates (>80% of work), shard it
       if let Some(dominant) = weights.iter()
           .find(|w| w.need_offsets as u64 * 100 / total_work > 80)
       {
           return ExecStrategy::IntraPackSharded { pack_id: dominant.pack_id };
       }

       // If enough packs for workers, use pack-parallel
       if weights.len() >= workers {
           return ExecStrategy::PackParallel;
       }

       // Otherwise, use simple pack-parallel (workers will be underutilized
       // but that's better than sharding tiny packs)
       ExecStrategy::PackParallel
   }
   ```

3. **Log strategy decision** for observability:
   ```rust
   tracing::debug!(
       strategy = ?selected_strategy,
       pack_count = weights.len(),
       workers = pack_exec_workers,
       "selected pack exec strategy"
   );
   ```

**Change points:**
- `src/git_scan/runner.rs:1127-1130` (replace simple heuristic)
- Add new `ExecStrategy` enum and `select_exec_strategy` function

**Metrics to validate:**
- Compare throughput on repos with varying pack distributions
- Log strategy decisions and correlate with performance

**Estimated complexity:** Medium (new decision logic, but contained to runner.rs)

**Implementation completed:** 2026-02-06
- Implemented a **stats-free** selector in `src/git_scan/runner_exec.rs` that uses only always-available `PackPlan` structure (`need_offsets`, `delta_deps`, `candidate_offsets`) and does **not** depend on `plan.stats`.
- Added adaptive shard sizing and explicit per-pack shard assignments:
  - `select_plan_shard_count(...)`
  - `shard_count_for_pack(...)`
  - `PackExecStrategy::IntraPackSharded { shard_counts }`
- Wired the selector into **both** scan mode pipelines:
  - `src/git_scan/runner_diff_history.rs` now selects with `select_pack_exec_strategy(pack_exec_workers, &plans)`.
  - `src/git_scan/runner_odb_blob.rs` now buffers underfilled plan sets (`pack_count < workers`) and applies the shared selector before execution.
- Removed hot-path runtime dependence on `plan.stats.candidate_count`; reservations now use `plan.candidate_offsets.len()` in pack-parallel workers.
- Added/updated tests:
  - `select_pack_exec_strategy_handles_serial_boundaries`
  - `select_pack_exec_strategy_prefers_pack_parallel_with_enough_plans`
  - `select_pack_exec_strategy_assigns_adaptive_shards`
  - `select_plan_shard_count_caps_dependency_heavy_plans`
  - Verified deterministic output via `diff_history_pack_exec_workers_preserve_deterministic_output`

---

## 5. [x] (Rank 5) Enable cluster-based sharding for cache locality â€” DE-SCOPED (2026-02-06)

**Decision:** Removed from roadmap and from the code model.

**Why de-scoped:**
- Recent profiling shows scan-path work dominates throughput, so this has low expected end-to-end impact.
- For measured repos, `forward_deps` is typically 0, so index sharding already preserves offset locality in practice.
- Cluster metadata was unused by execution and increased maintenance/API surface.

**Action taken in code:** Removed `Cluster`, `CLUSTER_GAP_BYTES`, and `PackPlan::clusters`.

---

## 6. [ ] (Rank 6) Batch external base resolution during planning

**Goal:** Prefetch external REF_DELTA bases before execution to avoid random I/O during the decode loop.

**Why this matters:** When a delta's base is in a different pack (REF_DELTA with external base), the current code calls `ExternalBaseProvider::load_base()` synchronously during execution (`pack_exec.rs:1091-1120`). For repos with many cross-pack references, this causes:
- Random I/O to other pack files
- Blocking the decode loop on I/O
- Cache pollution in the external pack

**Current behavior:**
```rust
// pack_exec.rs, during execution:
BaseLoc::External { oid } => {
    report.stats.external_base_calls += 1;
    match external.load_base(&oid) {
        // ... synchronous load
    }
}
```

**New behavior:**

1. **Collect external base OIDs during planning:**
   ```rust
   // In PackPlan, add:
   pub external_base_oids: Vec<OidBytes>,

   // In build_pack_plan_for_pack, collect:
   let external_base_oids: Vec<OidBytes> = delta_deps
       .iter()
       .filter_map(|dep| match dep.base {
           BaseLoc::External { oid } => Some(oid),
           _ => None,
       })
       .collect();
   ```

2. **Add batch prefetch trait:**
   ```rust
   // In pack_exec.rs:
   pub trait ExternalBaseProvider {
       fn load_base(&mut self, oid: &OidBytes) -> Result<Option<ExternalBase>, PackExecError>;

       /// Prefetch multiple bases in batch. Default is no-op.
       fn prefetch_bases(&mut self, _oids: &[OidBytes]) {}
   }
   ```

3. **Call prefetch before execution:**
   ```rust
   // In runner.rs, before execute_pack_plan_with_scratch:
   if !plan.external_base_oids.is_empty() {
       external.prefetch_bases(&plan.external_base_oids);
   }
   ```

4. **Implement prefetch in PackIo:**
   ```rust
   // In pack_io.rs:
   impl ExternalBaseProvider for PackIo<'_> {
       fn prefetch_bases(&mut self, oids: &[OidBytes]) {
           // For each OID, resolve to pack+offset via MIDX
           // Issue madvise(MADV_WILLNEED) on the byte ranges
           for oid in oids {
               if let Ok(Some((pack_id, offset))) = self.midx.resolve(oid) {
                   // Prefetch hint for this offset's region
                   self.prefetch_offset(pack_id, offset);
               }
           }
       }
   }
   ```

**Change points:**
- `src/git_scan/pack_plan.rs` (collect external OIDs)
- `src/git_scan/pack_plan_model.rs` (add field to PackPlan)
- `src/git_scan/pack_exec.rs` (add prefetch trait method)
- `src/git_scan/pack_io.rs` (implement prefetch)
- `src/git_scan/runner.rs` (call prefetch before execution)

**Metrics to validate:**
- Count `external_base_calls` and measure their latency
- Compare wall-clock time for repos with high external base rates

**Estimated complexity:** Medium-high (touches multiple modules, needs careful I/O handling)

---

## 7. [ ] (Rank 7) Add dependency-aware shard assignment

**Goal:** Keep delta base/dependent pairs in the same shard to avoid cross-shard cache misses.

**Why this matters:** When a delta and its base are assigned to different shards:
- The dependent shard may need to re-decode the base (fallback path)
- Or wait for the base shard to populate a shared cache (not currently implemented)

The `fallback_base_decodes` metric tracks this, and `fallback_chain_len_sum` shows the re-decode cost.

**Current behavior:** Sharding ignores delta dependencies entirely.

**New behavior:**

1. **Build dependency components using union-find:**
   ```rust
   // In runner.rs or new module:

   /// Groups indices into components where all deltas and their bases
   /// are in the same component.
   fn build_dependency_components(
       need_offsets: &[u64],
       delta_deps: &[DeltaDep],
   ) -> Vec<Vec<usize>> {
       let n = need_offsets.len();
       let mut parent: Vec<usize> = (0..n).collect();
       let mut rank: Vec<usize> = vec![0; n];

       // Build offset -> index map
       let offset_to_idx: HashMap<u64, usize> = need_offsets
           .iter()
           .enumerate()
           .map(|(i, &o)| (o, i))
           .collect();

       // Union base and dependent for each delta
       for dep in delta_deps {
           if let BaseLoc::Offset(base_offset) = dep.base {
               if let (Some(&dep_idx), Some(&base_idx)) = (
                   offset_to_idx.get(&dep.offset),
                   offset_to_idx.get(&base_offset),
               ) {
                   union(&mut parent, &mut rank, dep_idx, base_idx);
               }
           }
       }

       // Collect components
       let mut components: HashMap<usize, Vec<usize>> = HashMap::new();
       for i in 0..n {
           let root = find(&mut parent, i);
           components.entry(root).or_default().push(i);
       }

       components.into_values().collect()
   }

   fn find(parent: &mut [usize], i: usize) -> usize {
       if parent[i] != i {
           parent[i] = find(parent, parent[i]);
       }
       parent[i]
   }

   fn union(parent: &mut [usize], rank: &mut [usize], a: usize, b: usize) {
       let ra = find(parent, a);
       let rb = find(parent, b);
       if ra != rb {
           if rank[ra] < rank[rb] {
               parent[ra] = rb;
           } else if rank[ra] > rank[rb] {
               parent[rb] = ra;
           } else {
               parent[rb] = ra;
               rank[ra] += 1;
           }
       }
   }
   ```

2. **Shard by components:**
   ```rust
   fn shard_by_components(
       components: &[Vec<usize>],
       shards: usize,
   ) -> Vec<Vec<usize>> {
       // Greedy bin-packing: assign components to shards
       let mut shard_indices: Vec<Vec<usize>> = vec![Vec::new(); shards];
       let mut shard_sizes: Vec<usize> = vec![0; shards];

       // Sort components by size descending
       let mut sorted: Vec<_> = components.iter().collect();
       sorted.sort_by_key(|c| std::cmp::Reverse(c.len()));

       for component in sorted {
           let min_shard = shard_sizes
               .iter()
               .enumerate()
               .min_by_key(|(_, &s)| s)
               .map(|(i, _)| i)
               .unwrap_or(0);

           shard_indices[min_shard].extend(component.iter().copied());
           shard_sizes[min_shard] += component.len();
       }

       // Sort indices within each shard for cache locality
       for shard in &mut shard_indices {
           shard.sort_unstable();
       }

       shard_indices
   }
   ```

**Change points:**
- `src/git_scan/runner.rs` (add component building and sharding)
- Integrate with other locality strategies only if explicit locality metadata is reintroduced

**Metrics to validate:**
- Compare `fallback_base_decodes` before/after
- Compare `fallback_chain_len_sum` before/after

**Prerequisites:** Item 1 (instrumentation) to measure baseline fallback rates

**Estimated complexity:** Medium (union-find is standard, integration needs care)

---

## 8. [x] (Rank 8) Add per-cluster range prefetch hints â€” DE-SCOPED (2026-02-06)

**Decision:** Removed from roadmap.

**Reason:** This proposal depended on `plan.clusters`, which has been removed. If prefetching is revisited, it should derive ranges directly from execution indices instead of a separate clustering model.

---

## 9. [ ] (Rank 9) Implement smart cache admission policy

**Goal:** Only cache entries that are likely to be re-accessed (i.e., delta bases).

**Why this matters:** The current cache admits any entry that fits in a slot (`pack_cache.rs:148-154`). But non-delta blobs are typically accessed once and never again, wasting cache space.

**Current behavior:**
```rust
// pack_cache.rs:148-154
fn insert(&mut self, offset: u64, kind: ObjectKind, bytes: &[u8]) -> bool {
    if self.sets == 0 {
        return false;
    }
    if bytes.len() > self.slot_size as usize {
        return false;
    }
    // ... always admits if size fits
}
```

**New behavior:**

1. **Add "is base" hint to cache insert:**
   ```rust
   // In pack_cache.rs:
   pub fn insert_with_hint(
       &mut self,
       offset: u64,
       kind: ObjectKind,
       bytes: &[u8],
       is_delta_base: bool,
   ) -> bool {
       // Always admit delta bases
       if is_delta_base {
           return self.insert_internal(offset, kind, bytes);
       }

       // For non-bases, only admit if we have spare capacity
       // (e.g., less than 80% full in this set)
       let set = self.set_index(offset);
       let occupancy = self.set_occupancy(set);
       if occupancy < WAYS * 4 / 5 {
           return self.insert_internal(offset, kind, bytes);
       }

       false
   }
   ```

2. **Track base status during execution:**
   ```rust
   // In pack_exec.rs, when decoding:
   let is_base = plan.delta_dep_index
       .iter()
       .any(|&dep_idx| {
           dep_idx != NONE_U32 && matches!(
               plan.delta_deps[dep_idx as usize].base,
               BaseLoc::Offset(base_off) if base_off == offset
           )
       });
   // Or precompute a HashSet<u64> of base offsets during planning
   ```

3. **Simpler alternative - cache only if delta:**
   ```rust
   // Only cache entries that are delta bases or deltas themselves
   let should_cache = dep_idx != NONE_U32 || base_offsets.contains(&offset);
   if should_cache {
       cache.insert(offset, kind, bytes);
   }
   ```

**Change points:**
- `src/git_scan/pack_cache.rs` (add admission policy)
- `src/git_scan/pack_exec.rs` (pass hint to cache)
- `src/git_scan/pack_plan.rs` (optionally precompute base offset set)

**Metrics to validate:**
- Compare `cache_hits` / `cache_misses` with smarter admission
- Compare `cache_insert_rejects` and `cache_admission_skips`

**Estimated complexity:** Medium (needs careful integration with decode loop)

---

## 10. [ ] (Rank 10) Add adaptive parallelism based on pack statistics

**Goal:** Automatically choose the best execution strategy based on pack plan characteristics.

**Why this matters:** Different repos have different characteristics:
- Linux kernel: one huge pack, many candidates â†’ intra-pack sharding
- Monorepo with history: many medium packs â†’ pack-parallel
- Fresh clone: one pack with sparse candidates â†’ single-threaded may be fastest

**Implementation:**

1. **Define strategy enum:**
   ```rust
   #[derive(Debug, Clone, Copy)]
   pub enum PackExecStrategy {
       /// Single-threaded, sequential offset order
       SingleThreaded,
       /// One worker per pack, no intra-pack sharding
       PackParallel,
       /// Single pack sharded across workers
       IntraPackSharded,
   }
   ```

2. **Implement selection heuristic:**
   ```rust
   fn select_strategy(
       plans: &[PackPlan],
       workers: usize,
       total_candidates: usize,
   ) -> PackExecStrategy {
       if workers == 1 || total_candidates < 100 {
           return PackExecStrategy::SingleThreaded;
       }

       // Compute work distribution
       let total_need: usize = plans.iter()
           .map(|p| p.stats.need_count as usize)
           .sum();

       // If single pack has >80% of work, shard it
       if let Some(dominant) = plans.iter().find(|p| {
           p.stats.need_count as usize * 100 / total_need > 80
       }) {
           // But only if span is large enough to benefit
           if dominant.stats.candidate_span > 64 * 1024 * 1024 {
               return PackExecStrategy::IntraPackSharded;
           }
       }

       // If enough packs, use pack-parallel
       if plans.len() >= workers {
           return PackExecStrategy::PackParallel;
       }

       // Default to pack-parallel even if underutilizing workers
       // (better than sharding tiny packs)
       PackExecStrategy::PackParallel
   }
   ```

3. **Log strategy for tuning:**
   ```rust
   tracing::info!(
       strategy = ?selected,
       plans = plans.len(),
       workers,
       total_candidates,
       "pack exec strategy selected"
   );
   ```

**Change points:**
- `src/git_scan/runner.rs` (add strategy selection, refactor execution paths)

**Prerequisites:** Items 2, 3, 4 should be implemented first so there are meaningful strategies to choose between.

**Estimated complexity:** Medium (refactoring to consolidate execution paths)

---

## 11. [ ] (Rank 11) Prefetch delta chain headers during planning

**Goal:** Reduce latency in `resolve_fallback_base` by prefetching delta chain headers.

**Why this matters:** When a cache miss triggers fallback base resolution (`pack_exec.rs:1513`), the code walks the delta chain synchronously:
```rust
loop {
    let header = pack.entry_header_at(current_offset, ...)?;
    match header.kind {
        EntryKind::OfsDelta { base_offset } => {
            // Push to stack, continue to base
            current_offset = base_offset;
        }
        // ...
    }
}
```

Each header read may page-fault if not in cache.

**New behavior:**

1. **During planning, record delta chain structure:**
   ```rust
   // In PackPlan, add:
   pub delta_chains: Vec<DeltaChain>,

   #[derive(Debug)]
   pub struct DeltaChain {
       pub offsets: Vec<u64>, // From dependent to ultimate base
   }
   ```

2. **Before execution, prefetch chain headers:**
   ```rust
   for chain in &plan.delta_chains {
       for &offset in &chain.offsets {
           // Prefetch just the header region (typically <64 bytes)
           prefetch_range(pack_bytes, offset as usize, 64);
       }
   }
   ```

**Caveats:**
- Adds memory overhead to PackPlan
- Planning already parses headers; this just prefetches for execution
- May not help if execution immediately follows planning (headers still warm)

**Metrics to validate:**
- Measure `fallback_resolve_nanos` with/without prefetch
- Profile page faults in `resolve_fallback_base`

**Estimated complexity:** Medium-high (adds complexity to planning, uncertain benefit)

---

## Implementation Order Recommendation

**Updated 2026-02-06** (instrumentation results + Item 4 completion):

### Original Plan (Pre-Instrumentation)
1. ~~Item 1 (Instrumentation)~~ - âœ… DONE
2. Item 2 (Cache sizing) - Quick win, fixes existing bug
3. Item 3 (Heuristic tuning) - Builds on instrumentation data
4. Item 4 (Cluster sharding) - ~~planned~~ de-scoped
5. Item 5 (External base prefetch) - Independent, can parallelize
6. Items 6-10 - Based on instrumentation findings

### Revised Plan (Post-Instrumentation + Cache Fix + Scan Breakdown)

Based on sub-stage profiling across 7 repos showing **"other" (scan_chunk_into orchestration + RingChunker memcpy) dominates** at 51-80% of scan time:

1. ~~Item 1 (Instrumentation)~~ - âœ… DONE
2. ~~Item 2 (Cache admission policy)~~ - âœ… DONE (2025-02-05) - Fixed 0% hit rate by relaxing admission policy
3. ~~Item 2.5 (Investigate sink_emit)~~ - âœ… DONE (2025-02-05) - Sub-stage instrumentation complete across 7 repos. Key finding: **51-80% of scan time is "other"** (RingChunker memcpy + scan_chunk_into orchestration), not Vectorscan or regex validation.
4. ~~Item 2.5a (RingChunker bypass for small blobs)~~ - âœ… DONE (2025-02-05) - 99.8-100% of blobs take bypass. Reduced "other" by 1-23% absolute time.
5. ~~Item 2.5b (Skip redundant capacity revalidation)~~ - âœ… DONE (2025-02-05) - `reset_for_scan` capacity checks skipped after first call. Reset overhead now 0.0%.
6. ~~Item 2.5c (Binary blob detection)~~ - âœ… DONE (2025-02-05) - NUL-byte detection skips 2-1,208 binary blobs per repo.
7. ~~Item 2.5d (Zero-hit chunk prefilter bypass)~~ - âœ… DONE (2026-02-05) - Hoists Vectorscan prefilter above reset. "Other" reduced 44-92% across 7 repos. sink_emit reduced 25-66%. **Most impactful single optimization.**
8. ~~Item 4 (Pack-level vs intra-pack parallelism heuristic)~~ - âœ… DONE (2026-02-06) - Shipped stats-free adaptive selector + per-pack shard assignments in both scan modes.
9. **Vectorscan pattern optimization** â€” Vectorscan prefilter is now the dominant cost (35-68%) for low-finding-density repos. Further scan-path optimization has diminishing returns.
10. Item 3 (Cache under-provisioning in sharding) - May still be relevant for large repos (49-51% hit rate)
11. Item 6 (External base prefetch) - fallback_resolve is 33-94s on large repos
12. Item 7 (Parallelism tuning) - Lower priority
13. Items 8-11 - Re-evaluate after addressing scan overhead

### Key Insights

*Based on 7-repo dataset: gitleaks, go-git, rocksdb, react, node, vscode, tensorflow.*

1. **sink_emit (scanning) is the dominant bottleneck** â€” not decode or cache operations. Optimizing the pack execution layer yields diminishing returns.
2. **Within scanning, the "other" category dominates** (51-80% across all 7 repos) â€” this is RingChunker overhead + scan_chunk_into orchestration, not Vectorscan or regex.
3. **Vectorscan prefilter is efficient** â€” 18-27% of scan time, inherent O(n) cost.
4. **Validation is proportional to findings, not blobs** â€” only significant when findings are dense (gitleaks: 44.8%, vscode: 19.8%).
5. **62-95% of chunks produce zero prefilter hits** â€” the per-chunk overhead for "nothing matched" is the dominant cost for most repos.
6. **Cache hit rate degrades at scale** â€” 49-51% for large repos (node, vscode, tensorflow) vs 67-71% for small repos, validating cache under-provisioning as a concern at scale.
7. **fallback_resolve becomes significant on large repos** â€” 33-90s absolute time, suggesting cache sizing and external base prefetch (items 3/6) become more impactful at scale.

After each item, re-run instrumentation to validate impact before proceeding:
```bash
./target/release/git_scan --perf-breakdown ../gitleaks
./target/release/git_scan --perf-breakdown ../rocksdb
./target/release/git_scan --perf-breakdown ../go-git
./target/release/git_scan --perf-breakdown ../react
./target/release/git_scan --perf-breakdown ../node
./target/release/git_scan --perf-breakdown ../vscode
./target/release/git_scan --perf-breakdown ../tensorflow
```
